---
title: "Linear and Non-Linear Models week2"
author: "Xingqi"
date: "January 19, 2015"
output: pdf_document
---

Code Your Own Optimizer in R
This project helps understanding how non-linear optimization of log-likelihood function by the Newton-Raphson method works

The project is due by 11:59 pm on the day of your following class

This assignment is individual

Use the formulas for the steps of Newton-Raphson method from the lecture notes to code a simple version of optimizer.

1. Create a test function that needs to be optimized.

For this project we use one-dimensional optimization, i.e. optimization with respect to only one variable.

Let the declaration of the function be my.Function<-function(my.X), where my.X is a scalar parameter with respect to which the optimization is done.

The function should cross the x-axis at least in one point.

For example, you can use

```{r}
my.Function<-function(my.X) {
  my.X^2*3-my.X*5-5
}

my.Function.derivative<- function(inputx){
        6*inputx-5
}
```

2. Write your optimizer.

Let the declaration of the optimizer function be 'my.Optimizer<-function(Start.Value,Function.To.Optimize,Epsilon)'
where
'Start.Value' is the initial guess for the optimizer,
'Function.To.Optimize' is the name of your test function that needs to be optimized,
'Epsilon' is the stopping criterion, a small number, for example, 0.001.

The function 'my.Optimizer' should repeat iterations of the Newton-Raphson algorithm while \(|x_{i+1}-x_{i}|>=\epsilon\), where \(x_{i+1}\) is the approximation obtained during the recent iteration and \(x_i\) is the approximation obtained during the previous operation.
You can use any of the loops in R, for example, while(cond) expr where cond is the condition of moving to the next iteration (\(|x_{i+1}-x_{i}|>=\epsilon\)) and expr is the sequence of the commands that need to be performed at each iteration.

```{r}

my.Optimizer<-function(Start.Value,Function.To.Optimize,Epsilon)
        {
        delta=1
        iteration=0
        x1=Start.Value
        
        while(delta >Epsilon){
                x2=x1
                x1=x2-(my.Function(x1))/(my.Function.derivative(x2))
                delta=abs(x1-x2) 
                iteration=iteration+1
                print(paste0(iteration,x1))
         }
        
xoutput<<-x1
}

```




3. Test the optimizer.

Use your optimizer with the test function. For example, use my.Optimizer(-5,my.Function,.001).
Make sure you calculate the answer manually to check the answer.
```{r}
x<-my.Optimizer(-5,my.Function,.001);x
```

Manual calculation.

```{r}
x1=-5
x2=x1-(my.Function(x1))/(my.Function.derivative(x1));x2
x3=x2-(my.Function(x2))/(my.Function.derivative(x2));x3
x4=x3-(my.Function(x3))/(my.Function.derivative(x3));x4
x5=x4-(my.Function(x4))/(my.Function.derivative(x4));x5
x6=x5-(my.Function(x5))/(my.Function.derivative(x5));x6
rbind(x,x6)
```

The results from my.optimizer and manual calculation are identical. This shows that my.optimizer works fine.

You can also test the optimizer by running uniroot().
For example, uniroot(my.Function,lower=-5,upper=+1)

```{r}
uniroot(my.Function,lower=-5,upper=+1)
```


The root returned by your optimizer should be the same as the output$root of the object returned by uniroot().

Try also to run optim(). Explain the difference between the two functions: uniroot() and optim().

```{r}
optim.output<-optim((-5),
my.Function,
method="L-BFGS-B")
optim.output
```

How can we reconsile their outputs?

The outputs from uniroot() and optim() are different in above situtation. Function uniroot() uses one dimensional root-finding based on Newton-Raphson approach, while optim() try to minimize a criterion function that is the sum of squared differences to find the point of the maximal likelihood. Using the result of uniroot(), I could map to the result of optim().
